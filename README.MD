# Investigating Search Algorithms In Game Setting

The scope of this project included simulating a game of double-dominoes and testing various search algorithins.

For an in-depth analysis of gaming search algorithms check out "Multiplayer Games: Algorithms and Approaches, Sturtevant, 2003"

--> https://webdocs.cs.ualberta.ca/~nathanst/papers/multiplayergamesthesis.pdf

## Key Challenges
The first thing to consider when implementing a search algorithm in a game is the evaluation function. This function is used in many search algorithms and will return a value range based on the current state of the board. Essentially, the goal is to search a tree (every node layer is a turn and every branch a possible move) and find a path that reaches a node that has a good evaluation for that player - evaluation being a value that relates to winning chances. For this to work the same evaluation function needs to run when it's an opponent’s turn on the tree to determine the best possible scores for the opponent as well. 

The challenge however is that asymmetric information is inherent to certain types of games. This limitation can be observed in card games, for example, but not in games such as chess or checkers. In chess Player A and B are given exactly the same information whilst playing, whereas in a card game (or in this case dominoes) there is an asymmetry in the information possessed by players, and the information held by both players is incomplete. In games with imperfect information, the evaluation function needs to work for all players not only the player whose information we have. In dominoes, the information all players have is how many tiles each player has, how many tiles are left to be picked up, and what has already been played. Hence the evaluation function in the "Node" class accounts for these factors. 

The next thing to consider is how to search the tree. There is no one-size-fits-all approach, and every game will have its own requirements for an algorithm. In a game such as dominoes one major issue is that higher up in the tree (when few moves have been played), the evaluation function will return very similar results for all the possible moves. This is because the nature of the game is that there is usually a move available for an opponent early on in the game and as the evaluation function mainly accounts for whether a player can play or not - and not whether they can play a "strong" move, strong being a high-value tile. Thus the evaluation function only reflects a good position when it is guaranteed that an opponent can't play, which realistically only occurs when the current player knows that there aren't the correct tiles available to play a certain move, and this only occurs when there are very few options available which is usually at a later stage in play. 

It is only when you get to the leaves of the tree (terminal nodes, i.e. last moves) that the function will start to return larger differences in evaluation. This is why algorithms such as MaxN search the whole depth of the tree. The issue then lies with time and complexity. A tree grows exponentially with the number of nodes equaling the number of branches to the power of depth. In the case of three-player dominoes the number of nodes can easily go up to 5^32 in the first few moves. A solution is to not search certain branches which is called pruning. Depending on the evaluation function and the nature of the game, this could lead to some success.

Pruning is technically always less optimal at finding the best move but can be more effective as a larger number of leaf nodes can be searched in a shorter period. When dealing with large numbers this can be beneficial. To prune a branch, however, the node score cannot rely on the terminal node evaluation as this would simply be a regular MaxN search but rather needs the evaluation function to return a value that signals a loss in the score. In games where the points are only added up at the end of the game, and we don't know what our opponent has in their hand this becomes challenging. 

As mentioned previously for this game my evaluation function relies heavily on the counting of tiles and the knowledge of tiles already played. This means that the evaluation result stays consistent until the later stage of the game. Thus, when attempting to prune using various algorithms, such as Speculative Pruning, Approximate Deep Search, and Best-Reply Search, there was a drop in performance in regards to winning percentage as pruned branches were pruned based on their evaluation relative to nodes on a similar level (which would always be similar due to the inaccuracy of the evaluation function).

Considering the above, my implementation of a search algorithm is quite a simple one where I set a node search limit on the MaxN algorithm. This results in pseudo-random play in the early stages, with a more accurate decision-making process in the later stages of the game. The details are discussed further down. 

Lastly, other search algorithms are more complicated to implement than plain MaxN so the drop in win percentage of these algorithms could be due to incorrect implementation and not the logic described above. I have included a text file with pseudo-code for the other algorithms called "MaxN Algorithms".

## Results
- With a basic strategy after 10 000 games AI_2 wins 32% of the time against three other AI_1 players which is above the random distribution of 25%. Against 5 other AI_1 players, AI_2 won 20% of the games (16% random distribution), against 2 other AI_1's it won 47% (33% random distribution), and against only 1 AI_1 it won 70% of the time.
- At 3000 node search limit after 1000 games AI_3 played equally to two random players. Thus node depth needs to be higher, or a better solution to node sorting and searching needs to be implemented. 

## Game Rules
Double Dominoes - also known as Mexican Train - is like dominoes with a few added rules. Firstly, the numbers go up to twelve. Each player has their own "Train" where they can play their tiles. The game starts with the double 12 placed as an opening piece in the middle. Players take turns building their trains, and can only place their tiles on their trains. If a player cannot play their train is "Opened", meaning other players can temporarily play on their train. If a player plays on their own "Open" train it is closed again, and once again only they can play on it. In addition to the players' trains, there is a "Sauce" train which is always open for any player to play on. If a player cannot play on their train but can play on another train, then their train remains closed. 

Lastly, if a double ("Gate") is played (e.g. a block where both sides of the block contain the same number of dots) by any player on any train then the game is frozen and normal play can only resume once another tile has been played on the double. Players take turns trying to play on the double and cannot play on any other trains until the "Gate" is opened. The player who played the double is the first person who must attempt to open it, and if a player cannot open the gate they must pick up. 

The round ends when a player has finished their tiles, or no one can play and there are no more tiles left to pick up. A player's score per round is the sum of numbers on their unplayed tiles. Play continues until a player hits 100 points, then whoever has the lowest score wins.

## The AI
### AI_1 
Plays randomly. I did this to create a benchmark to compare with.
### AI_2 
Plays somewhat strategically by building sequences in its hand and prioritizing where it plays. First, it will check for "Interrupting Moves", a move that limits other players ability to play. Currently, it only calls this function if the move would mean other players can't play at all. I played around with playing tiles that somewhat limit other player moves but after running some simulations noticed that this tactic only impacts outcomes in the last few turns, and if it takes priority over the sequence moves then it doesn't improve winning chances. Next, it prioritizes playing on its train using a sequence from its hand. The "best" sequence is selected from all possible generated sequences, the chosen sequence factors in sequence length, the number of doubles in sequence, and the total value of all the tiles in the sequence. Then it will prioritize playing high-value tiles on the "sauce" train, and, as a last resort, it will play on other players' trains.
### AI_3
In addition to the search algorithm, this AI uses a "Terminal()" function and a "Find_Best_Move()" function. Terminal works by checking whether a player has finished their tiles or checking if all tiles have been picked up and no one can play. The function to find the best move works by creating a child node with a new state for every possible move and then running the MaxN algorithm with a unique node as an input parameter. AI_3 currently uses the MaxN Algorithm with a search limit applied. The main reason for the search limit was so that it would run fast enough to do a test of win distribution vs the other AIs. The choice of this algorithm is explained in the introduction. The search limit results in it playing pseudo-randomly until the later stages of the game when search width/depth is not an issue as there are fewer moves to evaluate. The current search depth is set to 5000 which is relatively low, and a more precise number could be found with more testing. Ultimately it is a balance between optimizing for speed and optimizing for performance, and finding the perfect balance for a specific game is a time-intensive task and it did not seem fruitful to pursue.

## Program
### 'Node_Class.py'
- This class allows for the search algorithms to build a search tree. It does this by storing an array of children in the current node. Each node has some built-in functionality to help with searching. 'Give_Options()' returns all the available moves that the current node has, which in the case of an opponent player will return a list of theoretically possible moves since the current player does not know the pieces the opponent has. 'Evalaute_Board()' returns a value of the board in a range from 0 - 1, where 1 is losing and 0 is winning. It does this by calculating the number of points not played and averaging it across how many pieces a player has left. It then divides that number by the total amount of points left overall, which will always be a fraction of one. For the current player, it could be more refined, but this wouldn't work as a search algorithm needs to be able to evaluate an opponent’s "score" without knowing what pieces they have. 'New_State()' returns a new board based on a move made by a player, this is done so that a search algorithm can assign new evaluations when moving down the tree, it also does this when a player picks up. Lastly, it can also 'Create_Children_Nodes()' for the search algorithm, it does this by taking all possible moves creating a new state (node) for each move, and returning a list of node objects.
- Each node stores the current board, a score for the board, the current player, and children nodes.
- When implementing other algorithms such as Speculative Pruning more functionality needed to be added, things like parent nodes, "best" score, and also a slightly different implementation (note that I removed these as I decided to stick with the MaxN algorithm)

### "Main.py" --imports--> "Simulator.py"
- Main allows for changing the number of players and inputting the number of simulations to run. It also keeps track of player wins as that is what Simulator returns.

### "Simulator.py" --imports--> "Functions.py", "AI_'1|2|3|4'.py" (AI_2 imports the sequence class to store sequence objects)
- "Simulator.py" has various functions to keep track of scores and has the Start_Game() function which will run one game through its rounds. It calls various functions from "Functions.py" and the AI files. The simulator accounts for things like rotating player turns, checking for game-end conditions, checking for closed gates, and other basics for gameplay to work. Note that there is a match case used in Start_Game() by which it is possible to select which AI will use what method of play. It also has various comments which can be uncommented to print data to the console.

### "Functions.py" ----> Classes (Players, Pile, Trains)
- Functions uses these classes and initiates them when called by the simulator so the data is all passed through and "Simulator.py" doesn't need to import any classes. When calling the Closed_Gate() and calling the AI all the data is passed into those functions and returned to the simulator, there is probably a way to access the data more efficiently but I haven’t looked into it. 

## Notes

- I initially decided to use numpy instead of lists to store tiles. I thought the difference in speed would be negligible and, whilst lists would have been a lot simpler, I wanted to familiarise myself with the library. I later discovered that numpy is very slow when appending things to an array as it creates a new copy every time. With lists it is also a lot easier to index tiles as a list of tuples can be used, hence I decided to stick with lists.

- To compare the AI I had them initially play 10 000 games after each change I made. For some reason when there are only 2 players the simulation hangs at certain points, haven't investigated why that is, seems like it gets stuck in some loops.

- The sequence creator functions are a bit of a mess but it seems to work and I don't think it's worth optimizing as it doesn't seem to significantly slow the program.
